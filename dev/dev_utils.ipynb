{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sqlite3\n",
    "import unicodecsv as csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#import squarify\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from shapely.geometry import Point, MultiPoint, Polygon, LineString, MultiLineString\n",
    "from shapely.ops import nearest_points, split, linemerge,transform\n",
    "from shapely.geometry import mapping\n",
    "from shapely.geometry import shape\n",
    "from shapely.wkt import loads\n",
    "from rtree import index\n",
    "from operator import itemgetter\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import rtree\n",
    "import math\n",
    "from functools import reduce\n",
    "#from astropy.io.ascii.connect import read_csv\n",
    "from datetime import datetime\n",
    "import statistics as st\n",
    "\n",
    "#pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "#pd.set_option('display.max_rows', 50000)\n",
    "#pd.set_option('display.max_columns', 100)\n",
    "#pd.set_option('display.max_colwidth', -1)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cartoframes import CartoDataFrame\n",
    "#from cartoframes.viz import Layout,Map,Layer,basemaps\n",
    "#from cartoframes.viz.helpers import color_bins_layer, size_bins_layer,color_continuous_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leecat11(archivo):\n",
    "\n",
    "    print ('Leyendo CAT '+archivo+': Registro 11')\n",
    "    \n",
    "    df0 = pd.read_csv(archivo, delimiter=\"··\", header=None,encoding='latin-1')\n",
    "    \n",
    "    df0['reg'] = df0[0].str.slice(0,2)\n",
    "    \n",
    "    df11 = df0[df0['reg']=='11']\n",
    "    df11.is_copy = False\n",
    "    \n",
    "    df11['TREG11'] = \"11\"\n",
    "    df11['BI'] = df11[0].str.slice(28,30)\n",
    "    df11['PCAT'] = df11[0].str.slice(30,44)\n",
    "    df11['CODPROV']=df11[0].str.slice(50,52)\n",
    "    df11['NOMPROV']=df11[0].str.slice(52,77)\n",
    "    df11['CODDGC']=df11[0].str.slice(77,80)\n",
    "    df11['CODINE']=df11[0].str.slice(80,83)\n",
    "    df11['NOMMUN']=df11[0].str.slice(83,123)\n",
    "    df11['NOMENT']=df11[0].str.slice(123,153)\n",
    "    df11['PCODVIA']=df11[0].str.slice(153,158)\n",
    "    df11['PTIPVIA']=df11[0].str.slice(158,163)\n",
    "    df11['PNOMVIA'] = df11[0].str.slice(163,188)\n",
    "    df11['PPNP'] = df11[0].str.slice(188,192)\n",
    "    df11['PPLE'] = df11[0].str.slice(192,193)\n",
    "    df11['PSNP'] = df11[0].str.slice(193,197)\n",
    "    df11['PSLE'] = df11[0].str.slice(197,198)\n",
    "    df11['KM'] = df11[0].str.slice(198,203)\n",
    "    df11['BLO'] = df11[0].str.slice(203,207)\n",
    "    df11['CP'] = df11[0].str.slice(240,245)\n",
    "    df11['DM'] = df11[0].str.slice(245,247)\n",
    "    df11['CODMUNOR'] = df11[0].str.slice(247,250)\n",
    "    df11['CODZC'] = df11[0].str.slice(250,252)\n",
    "    df11['CODPOL'] = df11[0].str.slice(252,255)\n",
    "    df11['CODPARC'] = df11[0].str.slice(255,260)\n",
    "    df11['CODPARA'] = df11[0].str.slice(260,265)\n",
    "    df11['SUPP'] = df11[0].str.slice(295,305)\n",
    "    df11['SUPCT'] = df11[0].str.slice(305,312)\n",
    "    df11['SUPCSR'] = df11[0].str.slice(312,319)\n",
    "    df11['SUPCBR'] = df11[0].str.slice(319,326)\n",
    "    df11['SUPCUB'] = df11[0].str.slice(326,333)\n",
    "    df11['COORX'] = df11[0].str.slice(333,342)\n",
    "    df11['COORY'] = df11[0].str.slice(342,352)\n",
    "    df11['RCBICE'] = df11[0].str.slice(581,601)\n",
    "    df11['DEN_BICE'] = df11[0].str.slice(601,666)\n",
    "    df11['SRS'] = df11[0].str.slice(666,323)\n",
    "    df11.drop(df11.columns[[0, 1]], axis = 1, inplace = True)\n",
    "    print('Hecho')\n",
    "    #display(df11.head(500))\n",
    "    return df11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leecat13(archivo):\n",
    "\n",
    "    print ('Leyendo CAT '+archivo+': Registro 13')\n",
    "    \n",
    "    df0 = pd.read_csv(archivo, delimiter=\"··\",header=None,encoding='latin-1')\n",
    "    \n",
    "    df0['reg'] = df0[0].str.slice(0,2)\n",
    "    \n",
    "    df13 = df0[df0['reg']=='13']\n",
    "    df13.is_copy = False\n",
    "    \n",
    "    df13['TREG13'] = \"13\"\n",
    "    df13['CLASUC'] = df13[0].str.slice(28,30)\n",
    "    df13['PCAT'] = df13[0].str.slice(30,44)\n",
    "    df13['COD_UC'] = df13[0].str.slice(44,48)\n",
    "    df13['ANOCONS'] = df13[0].str.slice(295,299)\n",
    "    df13['ANOEXAC'] = df13[0].str.slice(299,300)\n",
    "    df13['SUP_UC'] = df13[0].str.slice(300,307)\n",
    "    df13['LONG_FA'] = df13[0].str.slice(307,312)\n",
    "    df13['COD_UCM'] = df13[0].str.slice(409,413)\n",
    "    df13.drop(df13.columns[[0, 1]], axis = 1, inplace = True)\n",
    "    print('Hecho')\n",
    "    #display(df13.head(500))\n",
    "    return df13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leecat14(archivo):\n",
    "\n",
    "    print ('Leyendo CAT '+archivo+': Registro 14')\n",
    "    \n",
    "    df0 = pd.read_csv(archivo, delimiter=\"··\",header=None,encoding='latin-1')\n",
    "    \n",
    "    df0['reg'] = df0[0].str.slice(0,2)\n",
    "    \n",
    "    df14 = df0[df0['reg']=='14']\n",
    "    df14.is_copy = False\n",
    "    \n",
    "    df14['TREG14'] = \"14\"\n",
    "    df14['PCAT'] = df14[0].str.slice(30,44)\n",
    "    df14['ORD_ELC'] = df14[0].str.slice(44,48)\n",
    "    df14['NCBI_PC'] = df14[0].str.slice(50,54)\n",
    "    df14['COD_UC'] = df14[0].str.slice(54,58)\n",
    "    df14['BLO'] = df14[0].str.slice(58,62)\n",
    "    df14['ESC'] = df14[0].str.slice(62,64)\n",
    "    df14['PLANTA'] = df14[0].str.slice(64,67)\n",
    "    df14['PUERTA'] = df14[0].str.slice(67,70)\n",
    "    df14['CDD'] = df14[0].str.slice(70,73)\n",
    "    df14['TIPREF'] = df14[0].str.slice(74,74)\n",
    "    df14['ANOREF'] = df14[0].str.slice(74,78)\n",
    "    df14['ANOEFE'] = df14[0].str.slice(78,82)\n",
    "    df14['LOCINT'] = df14[0].str.slice(82,83)\n",
    "    df14['SUPT_CAT'] = df14[0].str.slice(83,90)\n",
    "    df14['SUPTP'] = df14[0].str.slice(90,97)\n",
    "    df14['SUPOP'] = df14[0].str.slice(97,104)\n",
    "    df14['TIPOL'] = df14[0].str.slice(104,109)\n",
    "    df14['MODR'] = df14[0].str.slice(111,114)\n",
    "    df14.drop(df14.columns[[0, 1]], axis = 1, inplace = True)\n",
    "    print('Hecho')\n",
    "    #display(df14.head(500))\n",
    "    return df14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leecat15(archivo):\n",
    "\n",
    "    print ('Leyendo CAT '+archivo+': Registro 15')\n",
    "    \n",
    "    df0 = pd.read_csv(archivo, delimiter=\"··\",header=None,encoding='latin-1')\n",
    "    \n",
    "    df0['reg'] = df0[0].str.slice(0,2)\n",
    "    \n",
    "    df15 = df0[df0['reg']=='15']\n",
    "    df15.is_copy = False\n",
    "    \n",
    "    df15['TREG15'] = \"15\"\n",
    "    df15['CODDGC']=df15[0].str.slice(25,28)\n",
    "    df15['CLASBI'] = df15[0].str.slice(28,30)\n",
    "    df15['PCAT'] = df15[0].str.slice(30,44)\n",
    "    df15['NCBI_PC'] = (df15[0].str.slice(44,48)).str.zfill(4)\n",
    "    df15['PCC'] = df15[0].str.slice(48,49)\n",
    "    df15['SCC'] = df15[0].str.slice(49,50)\n",
    "    df15['NFBI'] = df15[0].str.slice(50,58)\n",
    "    df15['IDBI'] = df15[0].str.slice(58,73)\n",
    "    df15['NFR'] = df15[0].str.slice(73,92)\n",
    "    df15['CODVIA']=df15[0].str.slice(195,200)\n",
    "    df15['TIPVIA']=df15[0].str.slice(200,205)\n",
    "    df15['NOMVIA'] = df15[0].str.slice(205,230).str.strip()\n",
    "    df15['PNP'] = df15[0].str.slice(230,234)\n",
    "    df15['PLE'] = df15[0].str.slice(234,235)\n",
    "    df15['SNP'] = df15[0].str.slice(235,239)\n",
    "    df15['SLE'] = df15[0].str.slice(239,240)\n",
    "    df15['KM'] = df15[0].str.slice(240,245)\n",
    "    df15['BLO'] = df15[0].str.slice(245,249)\n",
    "    df15['ESC'] = df15[0].str.slice(249,251)\n",
    "    df15['PLANTA'] = df15[0].str.slice(251,254)\n",
    "    df15['PUERTA'] = df15[0].str.slice(254,257)\n",
    "    df15['NOBI'] = df15[0].str.slice(367,371)\n",
    "    df15['ANOANT'] = df15[0].str.slice(371,375)\n",
    "    df15['CDD_BI'] = df15[0].str.slice(427,428)\n",
    "    df15['SUPCON'] = df15[0].str.slice(441,451)\n",
    "    df15['SUPNCNH'] = df15[0].str.slice(451,461)\n",
    "    df15['COEFPP'] = df15[0].str.slice(461,470)\n",
    "    df15.drop(df15.columns[[0, 1]], axis = 1, inplace = True)\n",
    "    print('Hecho')\n",
    "    #display(df15.head(500))\n",
    "    return df15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df (df):\n",
    "    original_cols = df.columns\n",
    "    num = ['PNP','SNP','KM','SUPP','SUPCT','SUPCSR','SUPCBR','SUPCUB','COORX','COORY','ANOCONS','SUP_UC','LONG_FA','ORD_ELC','ANOREF','ANOEFE','SUPT_CAT','SUPTP','SUPOP','ANOANT','SUPCON','SUPNCNH']\n",
    "    df = df.apply(lambda x: x.str.strip())\n",
    "    for col in df.columns:\n",
    "        if col in num:\n",
    "            df[col]=df[col].astype('int64')\n",
    "        else:\n",
    "            pass\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_and_drop(data1,data2,op,join_list):\n",
    "    df = pd.merge(data1 , data2 ,how = op, on = join_list , suffixes = (\"\",\"_r\"))\n",
    "    df = df[[col for col in df.columns if col[-2:] != \"_r\"]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_and_interpolated_vertices(data,chunk,kept_attributes):\n",
    "    \n",
    "    mod_dataset = []\n",
    "    re_mod_dataset = []\n",
    "    for row in tqdm(data.iterrows()):\n",
    "        lin = row[1]['geometry']\n",
    "        line_attributes = row[1][kept_attributes].to_dict()\n",
    "        vertices = MultiPoint(list(lin.coords))\n",
    "        coord = tuple(lin.coords)\n",
    "        \n",
    "        segments ={}\n",
    "        for position in range(len(coord)-1):\n",
    "            segment = {position+1:LineString([Point(coord[position]),Point(coord[position+1])]).wkt}\n",
    "            segments.update(segment)\n",
    "\n",
    "        heights = (max(coord[0][2],coord[-1][2]),min(coord[0][2],coord[-1][2]))\n",
    "        height = heights[0]-heights[1]\n",
    "        start = [tup for tup in coord if heights[1] in tup][0]\n",
    "        end = [tup for tup in coord if heights[0] in tup][0]\n",
    "        long = lin.length\n",
    "        slope = height/long\n",
    "        if long < chunk+20:\n",
    "            parts = 1\n",
    "        if long >= chunk+20:\n",
    "            parts = round(long/chunk)\n",
    "        slice_dist = long / parts\n",
    "        \n",
    "        interpolated =[]\n",
    "        if parts != 1:\n",
    "\n",
    "            for i in range(parts):\n",
    "                npoint = lin.interpolate(slice_dist*i)\n",
    "                interpolated.append(npoint)\n",
    "            interpolated = interpolated[1:]\n",
    "            \n",
    "            result =[]\n",
    "            for segment in segments:\n",
    "                segment_line = LineString(loads(segments[segment]))\n",
    "                segments[segment] = segment_line\n",
    "                for p in interpolated:\n",
    "                    if segment_line.distance(p) < 1e-8:\n",
    "                        segments[segment] = LineString([Point(segment_line.coords[0]),p,Point(segment_line.coords[1])])\n",
    "                    else:\n",
    "                        continue     \n",
    "\n",
    "            modified_line = MultiLineString([s for s in segments.values()])\n",
    "            modified_line = linemerge (modified_line)\n",
    "            breaks = MultiPoint(interpolated)\n",
    "            new_lines = list(split(modified_line,breaks))\n",
    "\n",
    "        else:\n",
    "            modified_line = lin\n",
    "            new_lines = [modified_line]\n",
    "            #print(\"Not Broken\")\n",
    "        \n",
    "        new_data = []\n",
    "        mod_original = []\n",
    "        for new_line in new_lines:\n",
    "            attributes = line_attributes\n",
    "            attr = {'slope':slope,'length':new_line.length,'geometry':new_line.wkt}\n",
    "            attr.update(attributes)\n",
    "            new_data.append(attr)\n",
    "\n",
    "        attributes = line_attributes\n",
    "        attr = {'slope':slope,'length':modified_line.length,'geometry':modified_line.wkt}\n",
    "        attr.update(attributes)\n",
    "        mod_original.append(attr)\n",
    "        \n",
    "        mod_dataset.append(pd.DataFrame(new_data))\n",
    "        re_mod_dataset.append(pd.DataFrame(mod_original))\n",
    "    \n",
    "    #The split dataset\n",
    "    fin1 = pd.concat(mod_dataset)\n",
    "    #the original dataset with added vertices\n",
    "    fin2 = pd.concat(re_mod_dataset)\n",
    "    return (fin1,fin2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_lines_at_points(data,points,kept_attributes,id_field):\n",
    "    #Note that vertices have to exist in the geometry\n",
    "\n",
    "    points = MultiPoint([Point(loads(p)) for p in points])\n",
    "    mod_dataset = []\n",
    "    for row in tqdm(data.iterrows()):\n",
    "        if row[1]['geometry'].startswith('MULTILINESTRING Z'):\n",
    "            continue\n",
    "        else:\n",
    "            lin = LineString(loads(row[1]['geometry']))\n",
    "        line_attributes = row[1][kept_attributes].to_dict()\n",
    "        vertices = MultiPoint(list(lin.coords))\n",
    "        coord = list(lin.coords)\n",
    "        heights = (max(coord[0][2],coord[-1][2]),min(coord[0][2],coord[-1][2]))\n",
    "        height = heights[0]-heights[1]\n",
    "        start = [tup for tup in coord if heights[1] in tup][0]\n",
    "        end = [tup for tup in coord if heights[0] in tup][0]\n",
    "        long = lin.length\n",
    "        slope = height/long\n",
    "        inner = MultiPoint([Point(p) for p in coord[1:-1] if Point(p).distance(points) < 1e-8])\n",
    "        new_data = []\n",
    "\n",
    "        if inner.is_empty:\n",
    "            new_lines = [lin]\n",
    "            pass\n",
    "        else:\n",
    "            new_lines = list(split(lin,inner))\n",
    "        for new_line in new_lines:\n",
    "            attributes = line_attributes\n",
    "            attr = {'slope':slope,'length':new_line.length,'geometry':new_line.wkt}\n",
    "            attr.update(attributes)\n",
    "            new_data.append(attr)\n",
    "\n",
    "        mod_dataset.append(pd.DataFrame(new_data))\n",
    "\n",
    "    split_df = pd.concat(mod_dataset)\n",
    "    return(split_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lines_endpoints(data):\n",
    "    points = []\n",
    "    for row in tqdm(data.iterrows()):\n",
    "        lin = loads(row[1]['geometry'])\n",
    "        l = list(lin.coords)\n",
    "        ends = [l[0],l[-1]]\n",
    "        points += ends\n",
    "    points = list(dict.fromkeys(points))\n",
    "    points = [Point(p) for p in points]\n",
    "    return (points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest(data,data_near,near_id):\n",
    "\n",
    "    idx = rtree.index.Index()\n",
    "\n",
    "    for fid, row in tqdm(data_near.iterrows()):\n",
    "        geometry = row['geometry'].bounds\n",
    "        idf = row[near_id]\n",
    "        idx.insert(fid, geometry)\n",
    "    \n",
    "   \n",
    "    adjacency = {}\n",
    "    \n",
    "    for fid, row in tqdm(data.iterrows()):\n",
    "        bpoint = row['geometry'].bounds\n",
    "        near_polys = {data_near.iloc[f][near_id]:data_near.iloc[f]['geometry'] for f in idx.nearest(bpoint, 1)}\n",
    "        point = Point(row['geometry'])\n",
    "        min_distance, min_poly = min(((poly.distance(point), poly) for poly in near_polys.values()), key=itemgetter(0))\n",
    "        closest_poly = list({k:v for k,v in near_polys.items() if v == min_poly}.keys())[0]\n",
    "        result = {fid:closest_poly}\n",
    "        adjacency.update(result)\n",
    "\n",
    "    adjacency = pd.DataFrame.from_dict(adjacency,orient='index',columns=[near_id])\n",
    "    \n",
    "    fdata = pd.merge(data,adjacency,how='left',left_index=True,right_index= True).reset_index()\n",
    "    \n",
    "    return fdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_totals(data,classes,values,city):\n",
    "    dfag = data.groupby([classes])[values].sum().reset_index()\n",
    "    dfag = dfag.sort_values(by=classes)\n",
    "    cmap = plt.get_cmap('tab20')\n",
    "    colors = cmap(np.linspace(0, 1 , len(dfag[classes])))\n",
    "    fig,ax = plt.subplots(nrows=1 , ncols=2 , figsize=(24,11) )\n",
    "    fig.suptitle('Total area by Land Use Class in '+cityname, fontsize = 24)\n",
    "    dfag = dfag.sort_values(by=values)\n",
    "    ax[0] = dfag.plot.bar(x= classes , y=values , rot=90 , ax=ax[0] , fontsize = 20, color = colors)\n",
    "    ax[0].set_title('Area of land Use Classes in sqrm',fontsize = 20)\n",
    "    y_formatter = matplotlib.ticker.ScalarFormatter(useOffset=False)\n",
    "    ax[0].yaxis.set_major_formatter(y_formatter)\n",
    "    ax[1] = squarify.plot(sizes=dfag[values], label=dfag[classes],  color = colors , alpha=.6 ,text_kwargs={'fontsize':20} )\n",
    "    ax[1].set_title('Proportion',fontsize = 20)\n",
    "    plt.axis('off')\n",
    "    fig.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora estamos probando con 4 para ajustar mas\n",
    "def tobler_speed_up(length,slope):\n",
    "    #enters in radians and meters, gives minutes\n",
    "    return round(4*(math.exp(-3.5*abs(slope+0.05)))*16.67,2)\n",
    "def tobler_speed_down(length,slope):\n",
    "    #enters in radians and meters, gives minutes   \n",
    "    return round(4*(math.exp(-3.5*abs(-(slope)+0.05)))*16.67,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_speed_by_slope(network,length_attribute,slope_attribute):\n",
    "    network['speed_up'] = network.apply(lambda row: tobler_speed_up (row[length_attribute],row[slope_attribute]),axis=1)\n",
    "    network['speed_down'] = network.apply(lambda row: tobler_speed_down (row[length_attribute],row[slope_attribute]),axis=1)\n",
    "    network['time_up'] = network[length_attribute] / network['speed_up']\n",
    "    network['time_down'] = network[length_attribute] / network['speed_down']\n",
    "    return(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_z_line(data_z):\n",
    "    for i,row in tqdm(data_z.iterrows()):\n",
    "        a = [coordinate[:2] for coordinate in list(row['geometry'].coords)]\n",
    "        data_z.at[i,'geometry'] = LineString(a)\n",
    "    return data_z\n",
    "def remove_z_point(data_z):\n",
    "    for i,row in tqdm(data_z.iterrows()):\n",
    "        #print(row['geometry'].coords[0][:2])\n",
    "        data_z.at[i,'geometry'] = Point(row['geometry'].coords[0][:2])\n",
    "    return data_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rates(data,rates_dict,origins):\n",
    "    rates = rates_dict\n",
    "    nrates = {\"N\"+k:v for k,v in rates.items()}\n",
    "    nrates = {k:v for k,v in nrates.items() if k == \"N\"+origins}\n",
    "    rates.pop(origins)\n",
    "    srates = {\"S\"+k:v for k,v in rates.items()}\n",
    "    data['origins'] = sum([data[k]*v for k,v in nrates.items()])\n",
    "    data['destinations'] = sum([data[k]*v for k,v in srates.items() if k in list(data.columns)])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_undirected_from_shp(shape_edges,shape_nodes,node_id_field,edge_id_field,weight_field,project_name):\n",
    "    \n",
    "    print(\"\\nGenerating Graph...\")\n",
    "    E = nx.read_shp(shape_edges, simplify = True , geom_attrs=True)\n",
    "    N = nx.read_shp(shape_nodes, simplify = False)\n",
    "    G = nx.Graph()\n",
    "    G = G.to_undirected()\n",
    "    print(\"\\nLoading Edge attributes\")\n",
    "    for data in tqdm(E.edges(data=True)):\n",
    "        if data[1]:\n",
    "            G.add_edge(data[0],data[1],ID=data[2][edge_id_field],weight=data[2][weight_field])\n",
    "    print(\"\\nLoading Node attributes\")\n",
    "    for data in tqdm(N.nodes(data=True)):\n",
    "        if data[1]:\n",
    "            G.add_node(data[0],attr_dict=data[1])\n",
    "    d = {k:v['attr_dict'][node_id_field] for k,v in dict(G.nodes(data=True)).items()}\n",
    "    G = nx.relabel_nodes(G, d, copy=False)\n",
    "    print(\"\\nGraph Ready\")\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walkable_trips(origins,destinations):\n",
    "    if origins == 0 and destinations == 0:\n",
    "        walkable =  0\n",
    "        unpaired =  0\n",
    "        unpaired_class =  'NULL'\n",
    "    elif origins <= destinations:\n",
    "        walkable =  origins\n",
    "        unpaired =  destinations - origins\n",
    "        unpaired_class =  'DESTINATIONS'\n",
    "    elif origins >= destinations:\n",
    "        walkable = destinations\n",
    "        unpaired =   origins - destinations \n",
    "        unpaired_class =  'ORIGINS'\n",
    "    return (walkable,unpaired,unpaired_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def only_walkable_trips(origins,destinations):\n",
    "    if origins == 0 and destinations == 0:\n",
    "        walkable =  0\n",
    "\n",
    "    elif origins <= destinations:\n",
    "        walkable =  origins\n",
    "\n",
    "    elif origins >= destinations:\n",
    "        walkable = destinations\n",
    "\n",
    "    return walkable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adjacency(graph,id_attribute,adjacency_radius,weight_attribute):\n",
    "\n",
    "    valid_nodes=[node for node in G.nodes( data = True )if node[1]]\n",
    "\n",
    "    adj = []\n",
    "    \n",
    "    for node in tqdm(valid_nodes):\n",
    "        data={}\n",
    "        data[id_attribute] = node[1]['attr_dict'][id_attribute]\n",
    "        sa_node = nx.ego_graph(G, node[0], radius = adjacency_radius, center=True, distance=weight_attribute)\n",
    "        sa_geometry = MultiPoint(list(sa_node.nodes)).wkt    \n",
    "        data['ADJPOINTS'] = sa_geometry\n",
    "        sa_if_nodes = [n[1]['attr_dict'][id_attribute] for n in sa_node.nodes( data = True) if n[1]]\n",
    "        data['ADJRID'] = ('|'.join(str(idn) for idn in sa_if_nodes))\n",
    "        adj.append(data)\n",
    "\n",
    "    df_adjacency = pd.DataFrame(adj)\n",
    "    \n",
    "    return df_adjacency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_in_context(data,adjacency,id_near,id_adjacency,adjacency_field,variables):\n",
    "    \n",
    "    adj = {row[id_adjacency] : [int(i) for i in row[adjacency_field].split(\"|\")] for i,row in adjacency.iterrows() }        \n",
    "    all_results=[]\n",
    "    for i,r in tqdm(adj.items()):\n",
    "        context = data.loc[data[id_near].isin(r)]\n",
    "        result = {id_near:i}\n",
    "        for v in variables:\n",
    "            total = {'T'+v : context[v].sum()}\n",
    "            result.update(total)\n",
    "        all_results.append(result)\n",
    "        \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
